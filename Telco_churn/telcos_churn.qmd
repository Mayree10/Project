
#Libraries

```{r}
set.seed(123)

library(dplyr)
library(tidyr)
library(caTools)
library(ROSE)
library(caret)
library(xgboost)
library(janitor)
library(pROC)
library(ggplot2)
library(tidyverse)
library(knitr)
library(kableExtra)
```

# Import Data

```{r}
churn_data <- read.csv(Telco-Customer-Churn.csv")

summary(churn_data)
sum(is.na(churn_data))


```

#Data Cleaning

```{r}
# Rename variables
churn_data <- churn_data %>% clean_names()

# Fix data types (numeric and make character factor)
churn_data$total_charges <- as.numeric(churn_data$total_charges)
churn_data$tenure <- as.numeric(churn_data$tenure)
churn_data$monthly_charges <- as.numeric(churn_data$monthly_charges)

churn_data <- churn_data %>%
  mutate_if(is.character, as.factor)


#Handle missing values
churn_data <- drop_na(churn_data)

#Trim white spaces
churn_data[] <- lapply(churn_data, function(x) if(is.character(x)) trimws(x) else x)

```

# Prepare Data for Modeling

```{r}
# Drop the ID column, not needed for prediction (not dropping, need it after)
#churn_data <- churn_data %>% 
  #select(-customer_id) 

# Convert Yes/No variables to binary

churn_data <- churn_data %>%
  mutate(across(where(~ all(.x %in% c("Yes", "No"))),
                ~ ifelse(.x == "Yes", 1, 0)))

# Split data into train and test data
set.seed(123)
split <- sample.split(churn_data$churn, SplitRatio = 0.7)
train_data <- subset(churn_data, split == TRUE)
test_data  <- subset(churn_data, split == FALSE)

# Remove customer_id (non-feature column)
train_data <- subset(train_data, select = -customer_id)
test_data  <- subset(test_data, select = -customer_id)

# Checking class imbalance
table(train_data$churn)
prop.table(table(train_data$churn))


# Taking care of class imbalance

# apply synthetic oversampling (#works like SMOTE)
train_balanced <- ROSE(churn ~ ., data = train_data, seed = 123)$data

# check balance
table(train_balanced$churn)
prop.table(table(train_balanced$churn))


```

# Build Logistic Regression Model

```{r}
train_balanced$churn <- as.factor(train_balanced$churn)

# Define training control
train_control <- trainControl(method = "cv", number = 10)

# Train GLM model with cross-validation
glm_model <- train(churn ~ ., data = train_balanced, method = "glm", family = "binomial", trControl = train_control)

# Print results
print(glm_model)


```

# Evaluating Logistic Regression Model and Making Predictions

```{r}
#Evaluating model
confusionMatrix(glm_model)

# Making predictions
glm_pred <- predict(glm_model, newdata = test_data)
predict_proba_glm <- predict(glm_model, newdata = test_data, type = "prob")[, "1"]

```

# Build a Random Forest Model

```{r}
#Train the model
rf_model <- train(churn ~ ., 
                  data = train_balanced, method = "rf", 
                  trControl = train_control)

# View the results
print(rf_model)

# Cross-validation results
rf_model$results

# Best tuning parameters
rf_model$bestTune
```

#Evaluating Random Forest Model and Making Predictions

```{r}
#Evaluating model
confusionMatrix(rf_model)

# Making predictions
rf_pred <- predict(rf_model, newdata = test_data)
predict_proba_rf <- predict(rf_model, newdata = test_data, type = "prob")[, "1"]

```

# Build an XGBOOST Model

```{r}
# Train the model
xgb_model <- train(
    churn ~ .,
    data = train_balanced,
    method = "xgbTree",
    trControl = train_control,
    tuneLength = 5
    )
print(xgb_model)

xgb_model$results
xgb_model$bestTune
```

#Evaluating XGBOOST Model and Making Predictions

```{r}
#Evaluating model
confusionMatrix(xgb_model)

# Making predictions
xgb_pred <- predict(xgb_model, newdata = test_data)
predict_proba_xgb <- predict(xgb_model, newdata = test_data, type = "prob")[, "1"]


```

# Comparing all three models

```{r}
test_data$churn <- as.factor(test_data$churn)
# GLM
glm_cm <- confusionMatrix(glm_pred, test_data$churn, positive = "1")

# Random Forest
rf_cm <- confusionMatrix(rf_pred, test_data$churn, positive = "1")

# XGBoost
xgb_cm <- confusionMatrix(xgb_pred, test_data$churn, positive = "1")

model_comparison <- data.frame(
  Model = c("Logistic Regression", "Random Forest", "XGBoost"),
  Accuracy = c(glm_cm$overall["Accuracy"],
               rf_cm$overall["Accuracy"],
               xgb_cm$overall["Accuracy"]),
  Precision = c(glm_cm$byClass["Precision"],
                rf_cm$byClass["Precision"],
                xgb_cm$byClass["Precision"]),
  Recall = c(glm_cm$byClass["Recall"],
             rf_cm$byClass["Recall"],
             xgb_cm$byClass["Recall"]),
  F1 = c(glm_cm$byClass["F1"],
             rf_cm$byClass["F1"],
             xgb_cm$byClass["F1"])
)

# Calculating AUC
glm_auc <- roc(test_data$churn, as.numeric(glm_pred))$auc
rf_auc  <- roc(test_data$churn, as.numeric(rf_pred))$auc
xgb_auc <- roc(test_data$churn, as.numeric(xgb_pred))$auc

#Adding AUC to the model comparison table
model_comparison$AUC <- c(glm_auc, rf_auc, xgb_auc)

print(model_comparison)

```

```{r}

#Getting optimal threshold for logistic regression model
roc_obj <- roc(test_data$churn, predict_proba_glm)
best_glm <- coords(roc_obj, "best", ret=c("threshold", "sensitivity", "specificity"))

# Getting optimal threshold for Random Forest model
roc_rf <- roc(test_data$churn, predict_proba_rf)
best_rf <- coords(roc_rf, "best", ret = c("threshold", "sensitivity", "specificity"))

# Getting optimal threshold for XGBoost model
roc_xgb <- roc(test_data$churn, predict_proba_xgb)
best_xgb <- coords(roc_xgb, "best", ret = c("threshold", "sensitivity", "specificity"))

#print(best_glm)
#print(best_rf)
#print(best_glm)
# Combine results into a data frame
threshold_summary <- data.frame(
  Model = c("Logistic Regression", "Random Forest", "XGBoost"),
  Threshold = c(as.numeric(best_glm["threshold"]),
                as.numeric(best_rf["threshold"]),
                as.numeric(best_xgb["threshold"])),
  Sensitivity = c(as.numeric(best_glm["sensitivity"]),
                  as.numeric(best_rf["sensitivity"]),
                  as.numeric(best_xgb["sensitivity"])),
  Specificity = c(as.numeric(best_glm["specificity"]),
                  as.numeric(best_rf["specificity"]),
                  as.numeric(best_xgb["specificity"]))
)
threshold_summary

```

# Using Best Thresholds for Prediction

```{r}

# Apply best thresholds to get new class predictions 
glm_pred_new <- ifelse(predict_proba_glm >= 0.5483724, "1", "0")
rf_pred_new  <- ifelse(predict_proba_rf  >= 0.4890000, "1", "0")
xgb_pred_new <- ifelse(predict_proba_xgb >= 0.4957135, "1", "0")


# Convert to factors with same levels as test data ---
glm_pred_new <- factor(glm_pred_new, levels = levels(test_data$churn))
rf_pred_new  <- factor(rf_pred_new,  levels = levels(test_data$churn))
xgb_pred_new <- factor(xgb_pred_new, levels = levels(test_data$churn))

# Compute confusion matrices
glm_cm <- confusionMatrix(glm_pred_new, test_data$churn, positive = "1")
rf_cm  <- confusionMatrix(rf_pred_new,  test_data$churn, positive = "1")
xgb_cm <- confusionMatrix(xgb_pred_new, test_data$churn, positive = "1")

# Summarize new metrics 
model_comparison_new <- data.frame(
  Model = c("Logistic Regression", "Random Forest", "XGBoost"),
  Accuracy  = c(glm_cm$overall["Accuracy"], rf_cm$overall["Accuracy"], xgb_cm$overall["Accuracy"]),
  Precision = c(glm_cm$byClass["Precision"], rf_cm$byClass["Precision"], xgb_cm$byClass["Precision"]),
  Recall    = c(glm_cm$byClass["Recall"], rf_cm$byClass["Recall"], xgb_cm$byClass["Recall"]),
  F1        = c(glm_cm$byClass["F1"], rf_cm$byClass["F1"], xgb_cm$byClass["F1"])
)

model_comparison_new

```

#Model Tuning

```{r}
# Define function
evaluate_thresholds <- function(true, prob, thresholds = seq(0.3, 0.7, by = 0.02)) {
  
  results <- data.frame()
  cms <- list()
  
  # Convert true to factor with levels "0" and "1" â€” caret expects factors
  true <- factor(true, levels = c("0", "1"))
  
  for (t in thresholds) {
    # Predict class: 1 if prob >= t, else 0
    pred <- ifelse(prob >= t, "1", "0") |> factor(levels = c("0", "1"))
    
    # Compute confusion matrix (positive class = "1")
    cm <- confusionMatrix(pred, true, positive = "1")
    
    # Extract metrics
    acc  <- cm$overall["Accuracy"]
    prec <- cm$byClass["Precision"]    
    rec  <- cm$byClass["Recall"]       
    f1   <- cm$byClass["F1"]
    
    # Handle NA (e.g., no positive predictions)
    prec <- ifelse(is.na(prec), 0, prec)
    f1   <- ifelse(is.na(f1), 0, f1)
    
    results <- rbind(results, data.frame(
      Threshold = t,
      Accuracy = acc,
      Precision = prec,
      Recall = rec,
      F1 = f1
    ))
    
    # Store confusion matrix table
    cms[[as.character(t)]] <- cm$table
  }
  
  return(list(metrics = results, confusion_matrices = cms))
}

# Evaluate models
glm_eval <- evaluate_thresholds(test_data$churn, predict_proba_glm)
rf_eval  <- evaluate_thresholds(test_data$churn, predict_proba_rf)
xgb_eval <- evaluate_thresholds(test_data$churn, predict_proba_xgb)

# Combine results
glm_eval$metrics$Model <- "Logistic Regression"
rf_eval$metrics$Model  <- "Random Forest"
xgb_eval$metrics$Model <- "XGBoost"  # Fixed typo

combined_results <- bind_rows(glm_eval$metrics, rf_eval$metrics, xgb_eval$metrics)

# Plot 
plot_data <- combined_results %>%
  pivot_longer(cols = c("Accuracy", "Precision", "Recall", "F1"),
               names_to = "Metric", values_to = "Value")

ggplot(plot_data, aes(x = Threshold, y = Value, color = Model)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  facet_wrap(~ Metric, scales = "free_y") +
  theme_minimal(base_size = 14) +
  labs(title = "Model Performance vs Threshold",
       x = "Threshold", y = "Metric Value", color = "Model") +
  theme(plot.title = element_text(face = "bold"), legend.position = "bottom")

```

```{r}
# Find best F1 threshold per model
best_thresholds <- combined_results %>%
  group_by(Model) %>%
  slice_max(order_by = F1, n = 1, with_ties = FALSE) %>%
  ungroup()

# Get the full metric values at those thresholds 
highlight_data <- plot_data %>%
  semi_join(best_thresholds, by = c("Model", "Threshold"))

# Plot with yellow highlights and dashed lines 
ggplot(plot_data, aes(x = Threshold, y = Value, color = Model)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  # Highlight best F1 points
  geom_point(data = highlight_data, 
             aes(x = Threshold, y = Value),
             size = 4, shape = 21, fill = "yellow", color = "black") +
  # Dashed line at best threshold
  geom_vline(data = best_thresholds,
             aes(xintercept = Threshold, color = Model),
             linetype = "dashed", size = 1) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(
    title = "Model Performance vs Threshold",
    subtitle = "Yellow points = Best F1 Score | Dashed line = Optimal Threshold",
    x = "Decision Threshold", 
    y = "Metric Value", 
    color = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom"
  )


```

# AUC Curves

```{r}
#Build ROC objects
roc_glm <- roc(test_data$churn, predict_proba_glm, levels = c("0", "1"), direction = "<")
roc_rf  <- roc(test_data$churn, predict_proba_rf,  levels = c("0", "1"), direction = "<")
roc_xgb <- roc(test_data$churn, predict_proba_xgb, levels = c("0", "1"), direction = "<")

#Extract AUC
auc_glm <- round(auc(roc_glm), 3)
auc_rf  <- round(auc(roc_rf), 3)
auc_xgb <- round(auc(roc_xgb), 3)

#Create named list with valid names
roc_list <- list(
  roc_glm = roc_glm,
  roc_rf  = roc_rf,
  roc_xgb = roc_xgb
)

#Update names with AUC
names(roc_list) <- c(
  paste0("Logistic Regression (AUC = ", auc_glm, ")"),
  paste0("Random Forest (AUC = ", auc_rf, ")"),
  paste0("XGBoost (AUC = ", auc_xgb, ")")
)

#Plot the curve
ggroc(roc_list, size = 1.2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  labs(
    title = "ROC Curves: Model Comparison",
    subtitle = "Higher AUC = Better ranking of churn risk",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 10)
  ) +
  guides(color = guide_legend(override.aes = list(size = 2)))
```

#Feature Importance

```{r}
#Random Forest Variable Importance
rf_importance <- varImp(rf_model, scale = FALSE)

# Extract as data frame
rf_df <- as.data.frame(rf_importance$importance) %>%
  rownames_to_column("Feature") %>%
  rename(Importance = Overall) %>%
  arrange(desc(Importance)) %>%
  mutate(Model = "Random Forest")

# Top 10 features
rf_top10 <- head(rf_df, 10)

#Plot
ggplot(rf_top10, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "#1f77b4", alpha = 0.8) +
  coord_flip() +
  labs(
    title = "Top 10 Predictors: Random Forest",
    x = "", y = "Importance Score"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 11)
  )

#XGBoost feature importance

xgb_importance <- xgb.importance(model = xgb_model$finalModel)

# Convert to data frame
xgb_df <- xgb_importance %>%
  as.data.frame() %>%
  select(Feature, Gain) %>%
  rename(Importance = Gain) %>%
  arrange(desc(Importance)) %>%
  mutate(Model = "XGBoost")

#Top 10 features
xgb_top10 <- head(xgb_df, 10)

#Plot
xgb_importance <- xgb.importance(model = xgb_model$finalModel)

xgb.plot.importance(importance_matrix = xgb_importance, top_n = 10)
title(main = "Top 10 Predictors: XGBoost (Gain)", 
      cex.main = 1.5, font.main = 2)
ggsave("rf_importance.png", width = 9, height = 6, dpi = 300)
ggsave("xgb_importance.png", width = 9, height = 6, dpi = 300)

```

#Final Table

```{r}
# Step 1: Extract metrics at default threshold (0.5) 
default_metrics <- model_comparison %>%
  select(Model, Accuracy, F1, AUC) %>%
  rename(
    Accuracy_0.5 = Accuracy,
    F1_0.5 = F1
  )

# Extract metrics at best threshold 
best_metrics <- model_comparison_new %>%
  select(Model, Accuracy, F1) %>%
  rename(
    Accuracy_best = Accuracy,
    F1_best = F1
  )

# Get best thresholds 
thresholds <- threshold_summary %>%
  select(Model, Threshold) %>%
  mutate(Threshold = round(Threshold, 3))

# Combine everything 
final_comparison <- default_metrics %>%
  left_join(best_metrics, by = "Model") %>%
  left_join(thresholds, by = "Model") %>%
  mutate(
    AUC = round(AUC, 3),
    across(c(Accuracy_0.5, F1_0.5, Accuracy_best, F1_best), ~ round(.x, 3))
  ) %>%
  select(
    Model,
    Accuracy_0.5, F1_0.5,
    Accuracy_best, F1_best,
    AUC,
    Threshold
  )

# Print table

final_comparison %>%
  kable(
    col.names = c(
      "Model", "Acc (0.5)", "F1 (0.5)",
      "Acc (Best)", "F1 (Best)", "AUC", "Best Threshold"
    ),
    caption = "Final Model Comparison: Default vs Optimal Threshold",
    align = "l"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center"
  ) %>%
  add_header_above(c(" " = 1, "Default Threshold" = 2, "Optimal Threshold" = 2, " " = 2),
                   bold = TRUE, background = "lightgray") %>%
  row_spec(0, bold = TRUE, background = "#f8f9fa")

```

# Business Impact

```{r}
#Average monthly charges
avg_monthly_revenue <- mean(churn_data$monthly_charges, na.rm = TRUE)
cat("Average Monthly Revenue (from data): $", round(avg_monthly_revenue, 2), "\n")

#Assume organization is spending $100 to retain a customer (offer/discount)
retention_cost <- 100  

#Test size
test_n <- nrow(test_data)

#Using predictions from random forest
predicted_churn <- as.numeric(xgb_pred_new== "1")
actual_churn <- as.numeric(test_data$churn == "1")

#True Positives (customers we correctly flag and could retain)
true_positives <- sum(predicted_churn * actual_churn)

#Total targeted
total_targeted <- sum(predicted_churn)

#Calculate impact on business
revenue_saved <- true_positives * avg_monthly_revenue * 12
cost_spent <- total_targeted * retention_cost
net_benefit <- revenue_saved - cost_spent
roi <- ifelse(cost_spent > 0, (net_benefit / cost_spent) * 100, 0)

#Print Results
cat("Business Impact: XGBoost (Best Threshold)")
cat("Test Set Size:", test_n, "customers\n")
cat("Actual Churn Rate:", round(mean(actual_churn) * 100, 1), "%\n")
cat("Predicted to Churn:", total_targeted, "(", round(mean(predicted_churn) * 100, 1), "%)\n")
cat("True Positives Retained:", true_positives, "\n\n")

cat("Avg Monthly Revenue (data): $", round(avg_monthly_revenue, 2), "\n")
cat("Annual Revenue Saved: $", round(revenue_saved), "\n")
cat("Retention Cost: $", round(cost_spent), "\n")
cat("Net Benefit: $", round(net_benefit), "\n")
cat("ROI: ", round(roi, 1), "%\n")

#Scale to Full Dataset
scale_factor <- nrow(churn_data) / test_n
full_revenue_saved <- revenue_saved * scale_factor
full_net_benefit <- net_benefit * scale_factor

cat("\n=== Scaled to Full Dataset (", nrow(churn_data), "customers) ===\n")
cat("Annual Revenue Saved: $", round(full_revenue_saved), "\n")
cat("Net Benefit: $", round(full_net_benefit), "\n")
```

# Identifying High risk Customers (Model deployment)

```{r}
# Predicting churn probability for all customers using Xgboost model
churn_data$churn_Prob <- predict(xgb_model, newdata = churn_data %>% select(-customer_id, -churn),
                                  type = "prob")[, "1"]

# Create risk level column
churn_data <- churn_data %>%
  mutate(
    Risk_Level = case_when(
      churn_Prob >= best_xgb$threshold ~ "High Risk",                    
      churn_Prob >= 0.4 ~ "Moderate Risk",                  
      TRUE ~ "Low Risk"                                   
    ) %>% factor(levels = c("Low Risk", "Moderate Risk", "High Risk"))
  )

# Selecting high risk customers
high_risk_customers <- churn_data %>%
  filter(Risk_Level == "High Risk") %>%
  arrange(desc(churn_Prob)) %>%
  select(customer_id, churn_Prob, Risk_Level, tenure, monthly_charges, contract, everything())

# Show top 10 high risk customers
high_risk_customers %>% head(10) %>%
  kable(caption = "Top 10 High-Risk Customers") %>%
  kable_styling()

#write.csv(high_risk_customers, "high_risk_customers.csv", row.names = FALSE)

```

# Revenue Estimate from High Risk Group

```{r}
high_risk <- high_risk_customers  

# Number of high risk customers
n_high_risk <- nrow(high_risk)
cat("High-Risk Customers Flagged:", n_high_risk, "\n")

# True churners in high-risk
true_churners_in_high_risk <- sum(high_risk$churn == "1")
cat("Actual Churners in High-Risk Group:", true_churners_in_high_risk, "\n")

# Assume we retain X% of them with offers (e.g., 30% success rate)
retention_success_rate <- 0.30
retained_customers <- round(true_churners_in_high_risk * retention_success_rate)
cat("Estimated Retained (30% success):", retained_customers, "\n")

# Avg monthly revenue (from data)
avg_monthly_revenue <- mean(churn_data$monthly_charges, na.rm = TRUE)
cat("Avg Monthly Revenue:", round(avg_monthly_revenue, 2), "\n")

# Annual revenue saved
annual_savings_high_risk <- retained_customers * avg_monthly_revenue * 12
cat("Annual Revenue Saved (High-Risk Group): $", round(annual_savings_high_risk), "\n")

# Cost of retention offers ($100 per targeted customer)
cost_per_offer <- 100
total_cost <- n_high_risk * cost_per_offer
net_savings <- annual_savings_high_risk - total_cost
roi_high_risk <- (net_savings / total_cost) * 100

cat("Total Cost (offers): $", total_cost, "\n")
cat("Net Savings: $", round(net_savings), "\n")
cat("ROI on High-Risk Campaign: ", round(roi_high_risk, 1), "%\n")
```
